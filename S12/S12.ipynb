{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytorch-lightning==2.0.2\n",
    "# pip install lightning-bolts==0.6.0.post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/PytorchLightning/lightning-bolts.git@master --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "# from torchmetrics import Accuracy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchmetrics.functional import accuracy\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \"./data/\")\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRes(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prepLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3), stride=1, padding=1, bias=False),## in_feature_map = 32, out_feature_map = 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Layer1_X1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =64, out_channels=128, kernel_size=(3,3), stride=1, padding=1, bias=False), #out_feature_map = 32\n",
    "            nn.MaxPool2d(2), #  out_feature_map = 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Layer1_R1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), stride=1, padding=1, bias=False),#  out_feature_map = 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), stride=1, padding=1, bias=False),#  out_feature_map = 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =128, out_channels=256, kernel_size=(3,3), stride=1, padding=1, bias=False), ##  out_feature_map = 16\n",
    "            nn.MaxPool2d(2),##  out_feature_map = 8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        self.Layer3_X2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels =256, out_channels=512, kernel_size=(3,3), stride=1, padding=1, bias=False), #out_feature_map = 8\n",
    "            nn.MaxPool2d(2), #  out_feature_map = 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Layer3_R2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=1, padding=1, bias=False),#  out_feature_map = 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=1, padding=1, bias=False),#  out_feature_map = 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )   \n",
    "        self.pool = nn.MaxPool2d(4) #  out_feature_map = 1\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.prepLayer(X)\n",
    "        X = self.Layer1_X1(X)\n",
    "        R1 = self.Layer1_R1(X)\n",
    "        X = R1+X\n",
    "        X = self.Layer2(X)\n",
    "        X = self.Layer3_X2(X)\n",
    "        R2 = self.Layer3_R2(X)\n",
    "        X = R2+X\n",
    "        X = self.pool(X)\n",
    "        X = X.view(-1,512)\n",
    "        X = self.fc(X)\n",
    "        X = F.log_softmax(X,dim=-1)\n",
    "        return X\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        loss = F.nll_loss(self(data), label)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x,y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss',loss,prog_bar=True)\n",
    "            self.log(f'{stage}_acc',acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, 'val')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters, self.hparams.lr)\n",
    "        steps_per_epoch=4500\n",
    "        schedular = OneCycleLR(optimizer=optimizer, max_lr = .1, epochs=self.trainer.max_epochs, steps_per_epoch=steps_per_epoch)\n",
    "        schedular_dict = {'scheduler':schedular,'interval':'step'}\n",
    "        return {'optimizer':optimizer, 'lr_scheduler':schedular_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir=PATH_DATASETS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`devices` selected with `CPUAccelerator` should be an int > 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m CustomRes()\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[1;32m      4\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,  \u001b[39m# limiting got iPython runs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     logger\u001b[39m=\u001b[39mCSVLogger(save_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39m[LearningRateMonitor(logging_interval\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m), TQDMProgressBar(refresh_rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)],\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, cifar10_dm)\n\u001b[1;32m     11\u001b[0m trainer\u001b[39m.\u001b[39mtest(model, datamodule\u001b[39m=\u001b[39mcifar10_dm)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/env-era/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py:69\u001b[0m, in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     trainer_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_kwargs)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_argparser\u001b[39m(\u001b[39mcls\u001b[39m: _ARGPARSE_CLS, arg_parser: Union[ArgumentParser, Namespace]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Namespace:\n\u001b[1;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse CLI arguments, required for custom bool types.\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     args \u001b[39m=\u001b[39m arg_parser\u001b[39m.\u001b[39mparse_args() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg_parser, ArgumentParser) \u001b[39melse\u001b[39;00m arg_parser\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/env-era/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:398\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m@_defaults_from_env_vars\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     inference_mode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    173\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    Customize every aspect of training via flags.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[39m        accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\")\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m            as well as custom accelerator instances.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[39m        accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[39m        amp_backend: The mixed precision backend to use (\"native\" or \"apex\").\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m            Default: ``'native''``.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[39m            .. deprecated:: v1.9\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m                Setting ``amp_backend`` inside the ``Trainer`` is deprecated in v1.8.0 and will be removed\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m                in v2.0.0. This argument was only relevant for apex which is being removed.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[39m        amp_level: The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m            if ``amp_backend`` is set to \"apex\".\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m            .. deprecated:: v1.8\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m                Setting ``amp_level`` inside the ``Trainer`` is deprecated in v1.8.0 and will be removed\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m                in v2.0.0.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[39m        auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m            trying to optimize initial learning for faster convergence. trainer.tune() method will\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m            set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m            To use a different key set a string instead of True with the key name.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[39m        auto_scale_batch_size: If set to True, will `initially` run a batch size\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m            finder trying to find the largest batch size that fits into memory.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m            The result will be stored in self.batch_size in the LightningModule\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m            or LightningDataModule depending on your setup.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m            Additionally, can be set to either `power` that estimates the batch size through\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m            a power search or `binsearch` that estimates the batch size through a binary search.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[39m        auto_select_gpus: If enabled and ``gpus`` or ``devices`` is an integer, pick available\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m            gpus automatically. This is especially useful when\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m            GPUs are configured to be in \"exclusive mode\", such\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m            that only one process at a time can access them.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[39m            .. deprecated:: v1.9\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m                ``auto_select_gpus`` has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m                Please use the function :func:`~lightning_fabric.accelerators.cuda.find_usable_cuda_devices`\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m                instead.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[39m        benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m            The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m            (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.Trainer.deterministic` is set\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m            to ``True``, this will default to ``False``. Override to manually set a different value.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[39m        callbacks: Add a callback or list of callbacks.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m        enable_checkpointing: If ``True``, enable checkpointing.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m            It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m            :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m            Default: ``True``.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[39m        check_val_every_n_epoch: Perform a validation loop every after every `N` training epochs. If ``None``,\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m            validation will be done solely based on the number of training batches, requiring ``val_check_interval``\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m            to be an integer value.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m            Default: ``1``.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[1;32m    243\u001b[0m \u001b[39m        default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m            Default: ``os.getcwd()``.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m            Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \u001b[39m        detect_anomaly: Enable anomaly detection for the autograd engine.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[39m        deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m            Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m            that don't support deterministic mode (requires PyTorch 1.11+). If not set, defaults to ``False``.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m \u001b[39m        devices: Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`,\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m            based on the accelerator type.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[39m        fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m            of train, val and test to find any bugs (ie: a sort of unit test).\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[39m        gpus: Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[39m            .. deprecated:: v1.7\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m                ``gpus`` has been deprecated in v1.7 and will be removed in v2.0.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m                Please use ``accelerator='gpu'`` and ``devices=x`` instead.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[39m        gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m            gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[39m        gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m            to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m            be set to ``\"norm\"``.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[39m        limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m            Default: ``1.0``.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m \u001b[39m        limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39m            Default: ``1.0``.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \n\u001b[1;32m    283\u001b[0m \u001b[39m        limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39m            Default: ``1.0``.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[39m        limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m            Default: ``1.0``.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m        logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m            the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m            ``False`` will disable logging. If multiple loggers are provided, local files\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m            (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of he first logger.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m            Default: ``True``.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[39m        log_every_n_steps: How often to log within steps.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39m            Default: ``50``.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \n\u001b[1;32m    298\u001b[0m \u001b[39m        enable_progress_bar: Whether to enable to progress bar by default.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m            Default: ``True``.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[39m        profiler: To profile individual steps during training and assist in identifying bottlenecks.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[39m        overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m            Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m \u001b[39m        plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[39m        precision: Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m            Can be used on CPU, GPU, TPUs, HPUs or IPUs.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m            Default: ``32``.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[39m        max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m            If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m            To enable infinite training, set ``max_epochs = -1``.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[39m        min_epochs: Force training for at least these many epochs. Disabled by default (None).\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[39m        max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39m            and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39m            ``max_epochs`` to ``-1``.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \n\u001b[1;32m    324\u001b[0m \u001b[39m        min_steps: Force training for at least these number of steps. Disabled by default (``None``).\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[39m        max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m            The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m            :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m            :class:`datetime.timedelta`.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[1;32m    331\u001b[0m \u001b[39m        num_nodes: Number of GPU nodes for distributed training.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m            Default: ``1``.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m        num_processes: Number of processes for distributed training with ``accelerator=\"cpu\"``.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m            Default: ``1``.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[39m            .. deprecated:: v1.7\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m                ``num_processes`` has been deprecated in v1.7 and will be removed in v2.0.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m                Please use ``accelerator='cpu'`` and ``devices=x`` instead.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m \u001b[39m        num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m            Set it to `-1` to run all batches in all validation dataloaders.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m            Default: ``2``.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[1;32m    345\u001b[0m \u001b[39m        reload_dataloaders_every_n_epochs: Set to a non-negative integer to reload dataloaders every n epochs.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m            Default: ``0``.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[39m        replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m            will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39m            train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39m            you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[39m        resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39m            no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m            training will start from the beginning of the next epoch.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[39m            .. deprecated:: v1.5\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m                ``resume_from_checkpoint`` is deprecated in v1.5 and will be removed in v2.0.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m                Please pass the path to ``Trainer.fit(..., ckpt_path=...)`` instead.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m        strategy: Supports different training strategies with aliases\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m            as well custom strategies.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[39m        sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m \u001b[39m        tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on (1)\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[39m            .. deprecated:: v1.7\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m                ``tpu_cores`` has been deprecated in v1.7 and will be removed in v2.0.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m                Please use ``accelerator='tpu'`` and ``devices=x`` instead.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[39m        ipus: How many IPUs to train on.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39m            Default: ``None``.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \n\u001b[1;32m    378\u001b[0m \u001b[39m            .. deprecated:: v1.7\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39m                ``ipus`` has been deprecated in v1.7 and will be removed in v2.0.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m                Please use ``accelerator='ipu'`` and ``devices=x`` instead.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \n\u001b[1;32m    382\u001b[0m \u001b[39m        track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m            Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39m            Default: ``-1``.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m        val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m            after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m            batches. An ``int`` value can only be higher than the number of training batches when\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m            ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m            across epochs or during iteration-based training.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m            Default: ``1.0``.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m        enable_model_summary: Whether to enable model summarization by default.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39m            Default: ``True``.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[39m        move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m            This can save some gpu memory, but can make training slower. Use with attention.\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m \u001b[39m            Default: ``False``.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[39m        multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m            In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m            and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m            reload when reaching the minimum length of datasets.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m            Default: ``\"max_size_cycle\"``.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \n\u001b[1;32m    406\u001b[0m \u001b[39m        inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39m            evaluation (``validate``/``test``/``predict``).\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    410\u001b[0m     Trainer\u001b[39m.\u001b[39m_log_api_event(\u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/env-era/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:157\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    152\u001b[0m         rank_zero_warn(\n\u001b[1;32m    153\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou passed `deterministic=True` and `benchmark=True`. Note that PyTorch ignores\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m torch.backends.cudnn.deterministic=True when torch.backends.cudnn.benchmark=True.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m \u001b[39m# TODO: move to gpu accelerator\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[39mif\u001b[39;00m benchmark \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39mbenchmark \u001b[39m=\u001b[39m benchmark\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbenchmark \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39mbenchmark\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/env-era/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:389\u001b[0m, in \u001b[0;36m_set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy_flag, \u001b[39m\"\u001b[39m\u001b[39mparallel_devices\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy_flag\u001b[39m.\u001b[39mparallel_devices:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy_flag\u001b[39m.\u001b[39mparallel_devices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    390\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    391\u001b[0m                 \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    392\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCPU parallel_devices set through \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy_flag\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m class,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but accelerator set to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag\u001b[39m}\u001b[39;00m\u001b[39m, please choose one device type\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m                 )\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/env-era/lib/python3.11/site-packages/pytorch_lightning/accelerators/cpu.py:48\u001b[0m, in \u001b[0;36mparse_devices\u001b[0;34m(devices)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accelerator device parsing logic.\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m devices \u001b[39m=\u001b[39m _parse_cpu_cores(devices)\n\u001b[0;32m---> 48\u001b[0m \u001b[39mreturn\u001b[39;00m devices\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/env-era/lib/python3.11/site-packages/lightning_fabric/accelerators/cpu.py:85\u001b[0m, in \u001b[0;36m_parse_cpu_cores\u001b[0;34m(cpu_cores)\u001b[0m\n\u001b[1;32m     82\u001b[0m     cpu_cores \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(cpu_cores)\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(cpu_cores, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m cpu_cores \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`devices` selected with `CPUAccelerator` should be an int > 0.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m cpu_cores\n",
      "\u001b[0;31mTypeError\u001b[0m: `devices` selected with `CPUAccelerator` should be an int > 0."
     ]
    }
   ],
   "source": [
    "\n",
    "model = CustomRes()\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)],\n",
    ")\n",
    "\n",
    "trainer.fit(model, cifar10_dm)\n",
    "trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-era",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
